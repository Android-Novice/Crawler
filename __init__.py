from datetime import datetime
import baidu_crawler
import jianshu_orm
import jianshu_crawler
from jianshu_crawler import start_crawling
import logging

from xkcd_crawler import comicPicsCrawling

logging.basicConfig(filename='D:\crawler.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s -%(message)s',datefmt='%m/%d/%Y %H:%M:%S %p')
logging.debug('Test....')

def filter_emoj(src_text):
    print(len(src_text))
    for i in src_text:
        print(i)
    for i in src_text.encode('utf-8'):
        print(i)

if __name__ == '__main__':
    # comicPicsCrawling()
    # import re
    baidu_crawler.baiduWebCrawling()
    #
    # try:
    #     # Wide UCS-4 build
    #     myre = re.compile(u'['
    #                       u'\U0001F300-\U0001F64F'
    #                       u'\U0001F680-\U0001F6FF'
    #                       u'\u2600-\u2B55]+',
    #                       re.UNICODE)
    # except re.error:
    #     # Narrow UCS-2 build
    #     myre = re.compile(u'('
    #                       u'\ud83c[\udf00-\udfff]|'
    #                       u'\ud83d[\udc00-\ude4f\ude80-\udeff]|'
    #                       u'[\u2600-\u2B55])+',
    #                       re.UNICODE)
    #
    # text = 'åˆšåˆšçŽ©ç®€ä¹¦ï¼Œå‘çŽ°è¿™é‡Œæ˜¯æ–‡è‰ºé’å¹´çš„å¤©å ‚é˜¿â˜º æˆ‘æ˜¯ä¸€åæ‘„å½±å¸ˆï¼Œä¹Ÿå–œæ¬¢æ–‡å­—ï¼Œå¦‚æžœå†™çš„æµ…æ˜¾ï¼Œæœ‰çš„æ—¶å€™ä¹Ÿä¼šè¯ç©·ï¼Œè°¢è°¢å¤§å®¶æŒ‡æ­£ï¼Œä¹Ÿå¯ä»¥å…³æ³¨æˆ‘çš„æ–°æµªå¾®åš@æ‘„å½±å¸ˆæ¨ä¹¦å¤ðŸ“·'
    # text1 = '4æœˆ14æ—¥Â·æ‰“å¡ðŸ”–èŠ·å¯¹ä½ è¯´'
    # text2 = 'ðŸ˜Šä¸åœ¨çš„æ—¶å€™å¯åŠ v15160324314'
    # text3 = 'å¤§å®¶æœ‰å–œæ¬¢èŒèŒå¤´åƒçš„å—ðŸ˜Šæƒ³è¦å±žäºŽè‡ªå·±çš„æ‰£1æ— å¿åˆ¶ä½œ'
    # text4 = 'é—ªäº®æŠ¤çœ¼å°ç™¾ç§‘å…³äºŽè¿Žé£Žæµæ³ªçš„å°ç§‘æ™®1âƒ£ï¸äººä»¬é•¿æ—¶é—´æ‚£æ²™çœ¼ã€æ…¢æ€§ç»“è†œç‚Žæˆ–æ…¢æ€§é¼»ç‚Žï¼Œå°±ä¼šç´¯åŠé¼»æ³ªç®¡ç²˜è†œï¼Œé€ æˆé¼»æ³ªç®¡é˜»å¡žã€‚ï¸æ³ªæ¶²ç§¯èšäºŽæ³ªå›Šä¸­ï¼Œçœ¼æ³ªå°±ä¼šä¸æ–­æµå‡ºã€‚ï¸å¦‚è¢«å†·é£Žä¸€å¹ï¼Œæ³ªè…ºåˆ†æ³Œä¼šå¢žå¤šï¼Œæ‰€ä»¥æµæ³ªä¹Ÿå°±ä¼šæ›´å¤šäº†ã€‚ï¸æ³ªä¸ºäººä½“äº”æ¶²ä¹‹ä¸€ï¼Œè‹¥ä¹…æµæ³ªä¸æ­¢ï¼Œéš¾è¾¨ç‰©è‰²ï¼Œç”šè‡³å¤±æ˜Žã€‚ï¸å¯è§è¿Žé£Žæµæ³ªå¹¶éžå°ç—…ï¼Œåº”åŠæ—©å°±æ²»ã€‚é—ªäº®æŠ¤çœ¼è´´ä¼šè®©ä½ æœ‰æ„æƒ³ä¸åˆ°çš„æ”¶èŽ·, '
    # text5 = 'å¿ƒç®€å• ä¸–ç•Œå°±ç®€å• å¹¸ç¦æ‰ä¼šç”Ÿé•¿ å¿ƒè‡ªç”± ç”Ÿæ´»å°±è‡ªç”± åˆ°å“ªéƒ½æœ‰å¿«ä¹ å°è¯•äº†ä¸€ç§æ–°çš„ç»˜ç”»æ„Ÿè§‰ï¼ŒåŸºæœ¬æ˜¯ä¸€æ°”å‘µæˆï¼Œç”»äº†ä¸€åŠæ‰æƒ³èµ·æ¥åŽ»æ‹æ‘„ä½œç”»è¿‡ç¨‹ï¼Œä¸çŸ¥é“å¤§å®¶ä¼šå–œæ¬¢ä¹ˆðŸ¤—'
    # text6 = 'ä¸ç”¨åç±½ä¸ç”¨å‰Šçš®ï¼Œæ¯”å…¶ä»–æ°´æžœåƒèµ·æ¥æ–¹ä¾¿å¤šäº†ï¼Œå¹¶ä¸”åƒå¤šå°‘éƒ½ä¸ä¼šä¸Šç« ï¼Œè¿˜æ˜Žç›®å…»è‚ï¼Œè€äººå°å­©éƒ½é€‚åˆåƒï¼ŒçœŸçš„æ˜¯å¥½å¤„å¤šå¤šå¤šå¤š......æƒ³åƒðŸ¤¤è¿™å­£èŠ‚å“ªæœ‰å–ï¼Ÿï¼Ÿï¼Ÿå“ˆå“ˆæˆ‘è¿™é‡Œæœ‰å“‡ï¼Œåˆšæ‘˜çš„...'
    #
    # str_text = u'0ðŸ“·1ðŸ”–2ðŸ˜Š3ðŸ˜Š4âƒ£ï¸5ï¸ï¸ï¸ï¸ðŸ¤—6ðŸ¤¤ä¸åœ¨çš„æ—¶å€™å¯åŠ v15160324314'
    #
    # list=list(str_text)
    # index =-1
    # for i in list:
    #     index +=1
    #     print(ord(i))
    #     if ord(i)>90000:
    #         list[index]=''
    # str_text = ''.join(list)
    # filter_emoj(str_text)

    # start_crawling()

    # jianshu_orm.init_mysql()
    # user = jianshu_orm.User()
    # user.id = '1'
    # user.like_count = 1
    # user.word_count = 10
    # user.article_count = 2
    # user.follower_count = 0
    # user.following_count = 1
    # user.name = 'test'
    # user.url = ''
    # user.image = 'about:blank'
    # user.articles = [
    #     jianshu_orm.Article('11', 'title', 'summary', '', datetime.now(), 100, 1000, 10000, 10, user.name),
    #     jianshu_orm.Article('211', '2title', '2summary', '', datetime.now(), 2100, 21000, 210000, 210, user.name)
    # ]
    # user.followers = [
    #     jianshu_orm.Follower('111111', 'follower', user.name),
    #     jianshu_orm.Follower('2111111', '2follower', user.name)
    # ]
    #
    # print('*********************************')
    # session = jianshu_orm.DBSession()
    # session.add(user)
    # session.commit()
    #
    # user.followers.append(jianshu_orm.Follower('31111111','Follower3', user.name))
    # user.articles.append(jianshu_orm.Article('311', '3title', '3summary', '', datetime.now(), 3100, 31000, 310000, 310, user.name))
    # session.add(user)
    # session.commit()

    # baidu_crawler.baiduWebCrawling('ç¾Žå¥³')
    pass
